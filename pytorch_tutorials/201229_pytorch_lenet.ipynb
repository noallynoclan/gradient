{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/4f/acf48b3a18a8f9223c6616647f0a011a5713a985336088d7c76f3a211374/torch-1.7.1-cp36-cp36m-manylinux1_x86_64.whl (776.8MB)\n",
      "\u001b[K     |████████████████████████████████| 776.8MB 64kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.0)\n",
      "Collecting typing-extensions (from torch)\n",
      "  Downloading https://files.pythonhosted.org/packages/60/7a/e881b5abb54db0e6e671ab088d079c57ce54e8a01a3ca443f561ccadb37e/typing_extensions-3.7.4.3-py3-none-any.whl\n",
      "Collecting dataclasses; python_version < \"3.7\" (from torch)\n",
      "  Downloading https://files.pythonhosted.org/packages/fe/ca/75fac5856ab5cfa51bbbcefa250182e50441074fdc3f803f6e76451fab43/dataclasses-0.8-py3-none-any.whl\n",
      "Installing collected packages: typing-extensions, dataclasses, torch\n",
      "  Found existing installation: torch 1.1.0\n",
      "    Uninstalling torch-1.1.0:\n",
      "      Successfully uninstalled torch-1.1.0\n",
      "Successfully installed dataclasses-0.8 torch-1.7.1 typing-extensions-3.7.4.3\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 20.3.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install torch --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from matplotlib import pyplot as plt\n",
    "from time import time\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataset = datasets.MNIST(root='dataset/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "test_dataset = datasets.MNIST(root='dataset/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, data_loader):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, targets) in enumerate(data_loader):\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            scores = model(data)\n",
    "            predictions = scores.argmax(1)\n",
    "            num_correct += (predictions == targets).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "    acc = num_correct / num_samples\n",
    "    return acc\n",
    "\n",
    "def train(model, data_loader, num_epochs, learning_rate):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        start_time = time()\n",
    "        for batch_idx, (data, targets) in enumerate(data_loader):\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            scores = model(data)\n",
    "            loss = model.loss(scores, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'epoch {epoch}, {time() - start_time:.1f}s: {accuracy(model, data_loader):.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, 8.9s: 93.8%\n",
      "epoch 1, 8.8s: 95.6%\n",
      "test: 95.2%\n"
     ]
    }
   ],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x): # [None, 1, 28, 28]\n",
    "        x = x.reshape(x.shape[0], -1) # [None, 784]\n",
    "        x = F.relu(self.fc1(x)) # [None, 64]\n",
    "        x = self.fc2(x) # [None, 10]\n",
    "        return x\n",
    "\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 2\n",
    "\n",
    "model = NN(input_size, num_classes)\n",
    "train(model, train_loader, num_epochs, learning_rate)\n",
    "print(f'test: {accuracy(model, test_loader):.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9)\n",
      "tensor([[-5.3349, -5.1922, -6.6372, -3.0437, -0.8401,  0.7449, -5.7820, -1.4924,\n",
      "         -0.2112,  4.7648]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(9, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def check(x, y, model):\n",
    "    plt.imshow(x[0, :, :], cmap='gray');\n",
    "    pred = model.forward(x.to(device))\n",
    "    print(y)\n",
    "    print(pred)\n",
    "    print(pred.argmax())\n",
    "    \n",
    "x, y = next(iter(train_loader))\n",
    "check(x[0], y[0], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, 11.2s: 96.6%\n",
      "epoch 1, 11.2s: 97.8%\n",
      "test: 97.9%\n"
     ]
    }
   ],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.AvgPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=(5, 5), stride=(1, 1), padding=(0, 0))\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=(5, 5), stride=(1, 1), padding=(0, 0))\n",
    "        self.linear1 = nn.Linear(120, 84)\n",
    "        self.linear2 = nn.Linear(84, 10)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv3(x)) # N x 120 x 1 x 1\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "lenet = LeNet()\n",
    "train(lenet, train_loader, num_epochs, learning_rate)\n",
    "print(f'test: {accuracy(lenet, test_loader):.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       " \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0min_channels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mout_channels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkernel_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstride\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpadding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdilation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgroups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpadding_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'zeros'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Applies a 2D convolution over an input signal composed of several input\n",
       "planes.\n",
       "\n",
       "In the simplest case, the output value of the layer with input size\n",
       ":math:`(N, C_{\\text{in}}, H, W)` and output :math:`(N, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})`\n",
       "can be precisely described as:\n",
       "\n",
       ".. math::\n",
       "    \\text{out}(N_i, C_{\\text{out}_j}) = \\text{bias}(C_{\\text{out}_j}) +\n",
       "    \\sum_{k = 0}^{C_{\\text{in}} - 1} \\text{weight}(C_{\\text{out}_j}, k) \\star \\text{input}(N_i, k)\n",
       "\n",
       "\n",
       "where :math:`\\star` is the valid 2D `cross-correlation`_ operator,\n",
       ":math:`N` is a batch size, :math:`C` denotes a number of channels,\n",
       ":math:`H` is a height of input planes in pixels, and :math:`W` is\n",
       "width in pixels.\n",
       "\n",
       "This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
       "\n",
       "* :attr:`stride` controls the stride for the cross-correlation, a single\n",
       "  number or a tuple.\n",
       "\n",
       "* :attr:`padding` controls the amount of implicit zero-paddings on both\n",
       "  sides for :attr:`padding` number of points for each dimension.\n",
       "\n",
       "* :attr:`dilation` controls the spacing between the kernel points; also\n",
       "  known as the à trous algorithm. It is harder to describe, but this `link`_\n",
       "  has a nice visualization of what :attr:`dilation` does.\n",
       "\n",
       "* :attr:`groups` controls the connections between inputs and outputs.\n",
       "  :attr:`in_channels` and :attr:`out_channels` must both be divisible by\n",
       "  :attr:`groups`. For example,\n",
       "\n",
       "    * At groups=1, all inputs are convolved to all outputs.\n",
       "    * At groups=2, the operation becomes equivalent to having two conv\n",
       "      layers side by side, each seeing half the input channels,\n",
       "      and producing half the output channels, and both subsequently\n",
       "      concatenated.\n",
       "    * At groups= :attr:`in_channels`, each input channel is convolved with\n",
       "      its own set of filters, of size:\n",
       "      :math:`\\left\\lfloor\\frac{out\\_channels}{in\\_channels}\\right\\rfloor`.\n",
       "\n",
       "The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:\n",
       "\n",
       "    - a single ``int`` -- in which case the same value is used for the height and width dimension\n",
       "    - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,\n",
       "      and the second `int` for the width dimension\n",
       "\n",
       "Note:\n",
       "\n",
       "     Depending of the size of your kernel, several (of the last)\n",
       "     columns of the input might be lost, because it is a valid `cross-correlation`_,\n",
       "     and not a full `cross-correlation`_.\n",
       "     It is up to the user to add proper padding.\n",
       "\n",
       "Note:\n",
       "\n",
       "    When `groups == in_channels` and `out_channels == K * in_channels`,\n",
       "    where `K` is a positive integer, this operation is also termed in\n",
       "    literature as depthwise convolution.\n",
       "\n",
       "    In other words, for an input of size :math:`(N, C_{in}, H_{in}, W_{in})`,\n",
       "    a depthwise convolution with a depthwise multiplier `K`, can be constructed by arguments\n",
       "    :math:`(in\\_channels=C_{in}, out\\_channels=C_{in} \\times K, ..., groups=C_{in})`.\n",
       "\n",
       "Note:\n",
       "    In some circumstances when using the CUDA backend with CuDNN, this operator\n",
       "    may select a nondeterministic algorithm to increase performance. If this is\n",
       "    undesirable, you can try to make the operation deterministic (potentially at\n",
       "    a performance cost) by setting ``torch.backends.cudnn.deterministic =\n",
       "    True``.\n",
       "    Please see the notes on :doc:`/notes/randomness` for background.\n",
       "\n",
       "\n",
       "Args:\n",
       "    in_channels (int): Number of channels in the input image\n",
       "    out_channels (int): Number of channels produced by the convolution\n",
       "    kernel_size (int or tuple): Size of the convolving kernel\n",
       "    stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
       "    padding (int or tuple, optional): Zero-padding added to both sides of\n",
       "        the input. Default: 0\n",
       "    padding_mode (string, optional): ``'zeros'``, ``'reflect'``,\n",
       "        ``'replicate'`` or ``'circular'``. Default: ``'zeros'``\n",
       "    dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n",
       "    groups (int, optional): Number of blocked connections from input\n",
       "        channels to output channels. Default: 1\n",
       "    bias (bool, optional): If ``True``, adds a learnable bias to the\n",
       "        output. Default: ``True``\n",
       "\n",
       "Shape:\n",
       "    - Input: :math:`(N, C_{in}, H_{in}, W_{in})`\n",
       "    - Output: :math:`(N, C_{out}, H_{out}, W_{out})` where\n",
       "\n",
       "      .. math::\n",
       "          H_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] - \\text{dilation}[0]\n",
       "                    \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n",
       "\n",
       "      .. math::\n",
       "          W_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] - \\text{dilation}[1]\n",
       "                    \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n",
       "\n",
       "Attributes:\n",
       "    weight (Tensor): the learnable weights of the module of shape\n",
       "        :math:`(\\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}},`\n",
       "        :math:`\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]})`.\n",
       "        The values of these weights are sampled from\n",
       "        :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
       "        :math:`k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}`\n",
       "    bias (Tensor):   the learnable bias of the module of shape\n",
       "        (out_channels). If :attr:`bias` is ``True``,\n",
       "        then the values of these weights are\n",
       "        sampled from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
       "        :math:`k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}`\n",
       "\n",
       "Examples:\n",
       "\n",
       "    >>> # With square kernels and equal stride\n",
       "    >>> m = nn.Conv2d(16, 33, 3, stride=2)\n",
       "    >>> # non-square kernels and unequal stride and with padding\n",
       "    >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n",
       "    >>> # non-square kernels and unequal stride and with padding and dilation\n",
       "    >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n",
       "    >>> input = torch.randn(20, 16, 50, 100)\n",
       "    >>> output = m(input)\n",
       "\n",
       ".. _cross-correlation:\n",
       "    https://en.wikipedia.org/wiki/Cross-correlation\n",
       "\n",
       ".. _link:\n",
       "    https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mFile:\u001b[0m           /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     Conv2d, ConvBn2d, Conv2d\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "? nn.Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
