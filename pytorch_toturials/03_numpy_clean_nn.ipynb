{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions\n",
    "## Sigmoid\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "S(x) &= \\frac{1}{1+e^{-x}} \\\\ \n",
    "\\frac{d}{dx}S(x) &= \\frac{d}{dx}\\frac{1}{1+e^{-x}} \\\\\n",
    "&= \\frac{e^{-x}}{(1+e^{-x})^2} \\\\ \n",
    "&= \\frac{1+e^{-x}}{(1+e^{-x})^2}-\\frac{1}{(1+e^{-x})^2} \\\\ \n",
    "&= \\frac{1}{1+e^{-x}}-\\frac{1}{(1+e^{-x})^2} \\\\ \n",
    "&= \\frac{1}{1+e^{-x}}\\big(1-\\frac{1}{1+e^{-x}}\\big) \\\\ \n",
    "&= S(x)\\big(1-S(x)\\big)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __call__(self, X):\n",
    "        \"\"\"\n",
    "        Shapes: [m, n] -> [m, n]\n",
    "        Example:\n",
    "            [[0,  1, -1]       [[0.5 , 0.73, 0.27]\n",
    "        f (  [2, -2,  3]] ) ->  [0.88, 0.12, 0.95]] \n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    \n",
    "    def grad(self, X):\n",
    "        \"\"\"\n",
    "        Shapes: [m, n] -> [m, n]\n",
    "        Example:\n",
    "            [[0,  1, -1]       [[0.25, 0.2, 0.2 ]\n",
    "        f (  [2, -2,  3]] ) ->  [0.1 , 0.1, 0.05]] \n",
    "        \"\"\"\n",
    "        sig = self(X)\n",
    "        return sig * (1 - sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __call__(self, X):\n",
    "        \"\"\"\n",
    "        Shapes: [m, n] -> [m, n]\n",
    "        Example:\n",
    "            [[0,  1, -1]       [[0, 1, 0]\n",
    "        f (  [2, -2,  3]] ) ->  [2, 0, 3]] \n",
    "        \"\"\"\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def grad(self, X):\n",
    "        \"\"\"\n",
    "        Shapes: [m, n] -> [m, n]\n",
    "        Example:\n",
    "            [[0,  1, -1]       [[0, 1, 0]\n",
    "        f (  [2, -2,  3]] ) ->  [1, 0, 1]] \n",
    "        \"\"\"\n",
    "        return np.maximum(0, np.sign(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACT = {'relu': Relu(), 'sigmoid': Sigmoid()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses\n",
    "* https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications/154880#154880\n",
    "\n",
    "## LogLoss\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{LogLoss}(y_t, y_p) &= -\\frac{1}{n}\\sum{\\big(y_t\\log{y_p} + (1-y_t)\\log{(1-y_p)}\\big)} \\\\\n",
    "\\frac{d}{dy_p}\\text{LogLoss}(y_t, y_p) &= -\\big(\\frac{y_t}{y_p} - \\frac{1-y_t}{1-y_p}\\big) \\\\ \n",
    "&= \\frac{1-y_t}{1-y_p} - \\frac{y_t}{y_p}\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogLoss():\n",
    "    def __call__(self, Y, Y_hat):\n",
    "        \"\"\"\n",
    "        Shapes: [m, n] -> 1\n",
    "        \"\"\"\n",
    "        loss =  Y * np.log(Y_hat) + (1 - Y) * np.log(1 - Y_hat)\n",
    "        return np.mean(-loss)\n",
    "    \n",
    "    def grad(self, Y, Y_hat):\n",
    "        \"\"\"\n",
    "        Shapes: [m, n] -> [m, n]\n",
    "        \"\"\"\n",
    "        return np.divide(1 - Y, 1 - Y_hat) - np.divide(Y, Y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "MSE(y_t, y_p) &= \\frac{1}{n}\\sum{(y_p-y_t)^2} \\\\\n",
    "\\frac{d}{dy_p}MSE(y_t, y_p) &= y_p - y_t\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE():\n",
    "    def __call__(self, Y, Y_hat):\n",
    "        \"\"\"\n",
    "        Shapes: [m, n] -> 1\n",
    "        \"\"\"\n",
    "        loss = (Y_hat - Y) ** 2\n",
    "        return np.mean(loss)\n",
    "    \n",
    "    def grad(self, Y, Y_hat):\n",
    "        \"\"\"\n",
    "        Shapes: [m, n] -> [m, n]\n",
    "        \"\"\"\n",
    "        return Y_hat - Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS = {'logloss': LogLoss(), 'mse': MSE()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, in_size, out_size, act='relu'):\n",
    "        \"\"\"\n",
    "        Shapes:\n",
    "        W: [in, out]\n",
    "        b: [1, out]\n",
    "        \"\"\"\n",
    "        self.W = np.random.randn(in_size, out_size)\n",
    "        self.b = np.random.randn(1, out_size)\n",
    "        self.act = ACT[act]\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Shapes: [m, in] × [in, out] + [1, out] = [m, out]\n",
    "        Example:\n",
    "        X × W + b = Z\n",
    "        [[0,  1, -1]    [[0, 1]               [[ 0,  1]\n",
    "             ...      ×  [2, 3]  + [[2, 3]] =    ...\n",
    "         [2, -2,  0]]    [4, 5]]               [-2, -1]]\n",
    "        \"\"\"                         \n",
    "        Z = X @ self.W + self.b\n",
    "        Y = self.act(Z)\n",
    "        return Y, Z\n",
    "    \n",
    "    def backward(self, X, Z, dX):\n",
    "        \"\"\"\n",
    "        Shapes:\n",
    "        X: [m, in]\n",
    "        Z: [m, out]\n",
    "        dX: [m, out]\n",
    "        dZ: [m, out]\n",
    "        dW: [in, m] × [m, out] -> [in, out]\n",
    "        db: [m, out] -> [1, out]\n",
    "        dX_prev: [m, out] × [out, in] -> [m, in]\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        dZ = dX * self.act.grad(Z)\n",
    "        dW = X.T @ dZ / m\n",
    "        db = np.mean(dZ, axis=0, keepdims=True)\n",
    "        dX_prev = dZ @ self.W.T\n",
    "        return dX_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, layers, loss='mse', seed=42):\n",
    "        num_layers = len(layers) - 1\n",
    "        self.layers = [[]] * num_layers\n",
    "        self.X = [[]] * num_layers\n",
    "        self.Z = [[]] * num_layers\n",
    "        self.dW = [[]] * num_layers\n",
    "        self.db = [[]] * num_layers\n",
    "        self.loss = LOSS[loss]\n",
    "        np.random.seed(seed)\n",
    "        for i in range(num_layers):\n",
    "            in_size, out_size = layers[i: i + 2]\n",
    "            self.layers[i] = Layer(in_size, out_size)\n",
    "            \n",
    "    def forward(self, X):\n",
    "        for i, l in enumerate(self.layers):\n",
    "            self.X[i] = X\n",
    "            X, self.Z[i] = l.forward(X)\n",
    "        return X\n",
    "\n",
    "    def backward(self, Y, Y_hat):\n",
    "        dX = self.loss.grad(Y, Y_hat)\n",
    "        for i, l in reversed(list(enumerate(self.layers))):\n",
    "            dX, self.dW[i], self.db[i] = l.backward(self.X[i], self.Z[i], dX)\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        for i, l in enumerate(self.layers):\n",
    "            l.W -= learning_rate * self.dW[i]\n",
    "            l.b -= learning_rate * self.db[i]\n",
    "\n",
    "    def train(self, X, Y, epochs, verbose=100, learning_rate=1e-5):\n",
    "        for i in range(epochs):\n",
    "            Y_hat = self.forward(X)\n",
    "            self.backward(Y, Y_hat)\n",
    "            if i % verbose == 0:\n",
    "                print({'eposh': i, 'loss': self.loss(Y, Y_hat), \n",
    "                       'grad': [np.mean(d) for d in self.dW]})\n",
    "            self.update(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eposh': 0, 'loss': 6599.363005885239, 'grad': [-5925.799150994647]}\n",
      "{'eposh': 1000, 'loss': 5.40122380186291, 'grad': [-2.317954562129738]}\n",
      "{'eposh': 2000, 'loss': 1.2720614736794869, 'grad': [-0.9136500455442104]}\n",
      "{'eposh': 3000, 'loss': 0.6344518005362033, 'grad': [-0.3591325039280515]}\n",
      "{'eposh': 4000, 'loss': 0.5359601597938106, 'grad': [-0.14119302348173338]}\n",
      "{'eposh': 5000, 'loss': 0.5207420630081447, 'grad': [-0.05553904003193775]}\n",
      "{'eposh': 6000, 'loss': 0.5183866106088224, 'grad': [-0.02187554449889954]}\n",
      "{'eposh': 7000, 'loss': 0.5180179557975881, 'grad': [-0.008645206705092787]}\n",
      "{'eposh': 8000, 'loss': 0.517956187590354, 'grad': [-0.003445452037034139]}\n",
      "{'eposh': 9000, 'loss': 0.517941822659721, 'grad': [-0.0014018549320747792]}\n"
     ]
    }
   ],
   "source": [
    "X = inputs = np.array([\n",
    "    [73, 67, 43], \n",
    "    [91, 88, 64], \n",
    "    [87, 134, 58], \n",
    "    [102, 43, 37], \n",
    "    [69, 96, 70]\n",
    "], dtype='float32')\n",
    "\n",
    "Y = np.array([\n",
    "    [56, 70], \n",
    "    [81, 101], \n",
    "    [119, 133], \n",
    "    [22, 37], \n",
    "    [103, 119]\n",
    "], dtype='float32')\n",
    "\n",
    "m = Model([3, 2])\n",
    "m.train(X, Y, 10000, 1000, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.19267347, -0.01633728, -0.34794431, -0.00444718],\n",
       "       [-0.03196527, -0.02690705,  0.73858809,  0.309309  ],\n",
       "       [-0.12634051,  0.05170876,  0.59796808,  0.18072154]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.layers[0].W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.24320262,  0.03142473],\n",
       "       [-0.09070897, -0.14123037],\n",
       "       [ 1.0060721 , -0.02257763],\n",
       "       [ 0.31407386, -0.14247482]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.layers[1].W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04790052,  0.01109226]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.layers[1].b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(3, 0), dtype=float64)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.zeros()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(3, 0), dtype=float64)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1] = np.array([2])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
